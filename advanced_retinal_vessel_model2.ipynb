{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d077b5",
   "metadata": {},
   "source": [
    "# Advanced Retinal Vessel Model 2\n",
    "This notebook contains the implementation of an advanced retinal vessel segmentation model using multi-scale features, transformer blocks, and attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26631798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "def multi_scale_features2(x, filters):\n",
    "    scales = []\n",
    "    for i, rate in enumerate([1, 2, 4, 8]):\n",
    "        scaled = Conv2D(filters, kernel_size=3, dilation_rate=rate, padding='same', activation='relu')(x)\n",
    "        scales.append(scaled)\n",
    "    return Concatenate()(scales)\n",
    "\n",
    "def transformer_block2(x, num_heads=4, ff_dim=256):\n",
    "    projected_x = Dense(ff_dim)(x)\n",
    "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim // num_heads)(projected_x, projected_x)\n",
    "    add_skip = Add()([attention, projected_x])\n",
    "    attention_output = LayerNormalization()(add_skip)\n",
    "    ff_output = Dense(ff_dim, activation=\"relu\")(attention_output)\n",
    "    ff_output = Dense(projected_x.shape[-1])(ff_output)\n",
    "    add_skip_ff = Add()([ff_output, attention_output])\n",
    "    return LayerNormalization()(add_skip_ff)\n",
    "\n",
    "def aspp_transformer_block2(x):\n",
    "    aspp = multi_scale_features2(x, 64)\n",
    "    transformer = transformer_block2(aspp, num_heads=4, ff_dim=x.shape[-1])\n",
    "    return transformer\n",
    "\n",
    "def attention_transformer_block2(x, g, filters):\n",
    "    g1 = Conv2D(filters, kernel_size=1, activation='relu')(g)\n",
    "    x1 = Conv2D(filters, kernel_size=1, activation='relu')(x)\n",
    "    psi = Add()([g1, x1])\n",
    "    psi = Conv2D(1, kernel_size=1, activation='sigmoid')(psi)\n",
    "    attention_applied = Multiply()([x, psi])\n",
    "    return transformer_block2(attention_applied, num_heads=4, ff_dim=filters)\n",
    "\n",
    "def advanced_retinal_vessel_model2(input_shape):\n",
    "    filters = [32, 64, 128, 256, 512]\n",
    "    inputs = Input(input_shape)\n",
    "    enc1 = multi_scale_features2(inputs, filters[0])\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(enc1)\n",
    "    enc2 = multi_scale_features2(pool1, filters[1])\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(enc2)\n",
    "    enc3 = multi_scale_features2(pool2, filters[2])\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(enc3)\n",
    "    enc4 = multi_scale_features2(pool3, filters[3])\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(enc4)\n",
    "    bottleneck = aspp_transformer_block2(pool4)\n",
    "    up4 = UpSampling2D(size=(2, 2))(bottleneck)\n",
    "    att4 = attention_transformer_block2(enc4, up4, filters[3])\n",
    "    dec4 = Conv2D(filters[3], kernel_size=(3, 3), padding='same', activation='relu')(Concatenate()([up4, att4]))\n",
    "    up3 = UpSampling2D(size=(2, 2))(dec4)\n",
    "    att3 = attention_transformer_block2(enc3, up3, filters[2])\n",
    "    dec3 = Conv2D(filters[2], kernel_size=(3, 3), padding='same', activation='relu')(Concatenate()([up3, att3]))\n",
    "    up2 = UpSampling2D(size=(2, 2))(dec3)\n",
    "    att2 = attention_transformer_block2(enc2, up2, filters[1])\n",
    "    dec2 = Conv2D(filters[1], kernel_size=(3, 3), padding='same', activation='relu')(Concatenate()([up2, att2]))\n",
    "    up1 = UpSampling2D(size=(2, 2))(dec2)\n",
    "    att1 = attention_transformer_block2(enc1, up1, filters[0])\n",
    "    dec1 = Conv2D(filters[0], kernel_size=(3, 3), padding='same', activation='relu')(Concatenate()([up1, att1]))\n",
    "    outputs = Conv2D(1, kernel_size=(1, 1), activation='sigmoid')(dec1)\n",
    "    model = models.Model(inputs, outputs, name=\"Advanced_Retinal_Vessel_Model2\")\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
